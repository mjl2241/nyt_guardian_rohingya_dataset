---
title: "Guardian API"
output: github_document
---

The following code is used to bring in data using the Guardian API

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# loading the packages:
library(dplyr) # for pipes and the data_frame function
library(rvest) # webscraping
library(stringr) # to deal with strings and to clean up our data
library(rvest)
library(tidyverse)
library(RJSONIO)
library (RCurl)
library(tidyr)
library(jsonlite)
library(httr)
library(lubridate)
```
#### Scrapping news articles on the Guardian using API


details: 
guardian_key <- "702f45d7-a2f9-4c88-8ba4-695cc9e9c275"
term <- "Rohingya+refugee"
term <- "Rohingya"
from-date <- "2010-01-01"
to-date <- "2021-01-01"
pagesize=200
```{r}
url <- paste0("https://content.guardianapis.com/search?q=rohingya&from-date=2010-01-01&to-date=2020-12-31&page-size=200&api-key=702f45d7-a2f9-4c88-8ba4-695cc9e9c275",sep="")

df <- fromJSON(url)
maxPages <- df$response$pages #check the number of pages
gu_pages_rohingya <- vector("list",length=maxPages)

for(i in 1:maxPages){
    gu_Search <- fromJSON(paste0(url, "&page=", i), flatten = TRUE) %>% data.frame() 
    gu_pages_rohingya[[i+1]] <- gu_Search 
    Sys.sleep(5)}
#I was getting errors more often when I waited only 1 second between calls. 5 seconds seems to work better.

guardian_rohingya <- rbind_pages(gu_pages_rohingya) %>%
    janitor::clean_names() 
save(guardian_rohingya,file="guardian_rohingya.Rdata")
```
#### data cleaning

Minor details:
- A search of articles from 2010/01/01 to 2021/01/01, "Rohingya" resulted in 1043 total articles.
- A search of articles from the same time frame, "Rohingya Refugee" produced 455 total articles.

Exploring and cleaning data frames
- keep only relevant columns
```{r message=FALSE, warning=FALSE}
colnames(guardian_rohingya) <- str_replace(colnames(guardian_rohingya),
                pattern='response.',replace='')
colnames(guardian_rohingya) <- str_replace(colnames(guardian_rohingya),
                pattern='response.results.',replace='')
colnames(guardian_rohingya) <- str_replace(colnames(guardian_rohingya),
                pattern='results.',replace='')
colnames(guardian_rohingya)
#keep relevant columns;
col_keep <- c("web_publication_date", "web_title", "web_url", "pillar_name")
guardian_rohingya_sub <-guardian_rohingya[col_keep]
```
#### Deleting irrelevant and duplicated articles
Based on the category column of the dataset, we can include just the News articles and delete others (opinions, arts ,lifestyle, sports). This resulted in a total 814 articles. 
```{r}
guardian_rohingya_sub =
  rename(guardian_rohingya_sub,
    date= web_publication_date,
    title = web_title,
    url =web_url,
    category = pillar_name
  )

unique(guardian_rohingya_sub[c("category")])

guardian_rohingya_news <-
  guardian_rohingya_sub %>%
  filter(category == "News")
guardian_rohingya_news[duplicated(guardian_rohingya_news$title),]
#8 duplicated articles based on title, however, manually looking at the date and title, these articles were all unique articles./
guardian_rohingya_news
save(guardian_rohingya_news,file="rohingya_news.Rdata")
```
Our resulting sample includes 817 articles 

#### sort by acending date and count publications per month/year.

```{r}
library(lubridate)
#arrange by date
guardian_rohingya_news$date <- as.Date(substr(guardian_rohingya_news$date,1,10))
guardian_rohingya_news <- 
  guardian_rohingya_news%>% 
  arrange(date)

#counting daily publications
guardian_rohingya_count <-
guardian_rohingya_news %>%
  group_by(date) %>%
  count(name="guardian_pub_count")
save(guardian_rohingya_count,file="rohingya_news.Rdata")
```
